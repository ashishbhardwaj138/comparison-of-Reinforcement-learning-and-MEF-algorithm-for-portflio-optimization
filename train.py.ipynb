{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c42758b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from tqdm import tqdm\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.policies.policy_saver import PolicySaver\n",
    "from tf_agents.agents.ddpg import actor_network\n",
    "from tf_agents.agents.ddpg import critic_network\n",
    "from tf_agents.agents.ddpg import ddpg_agent\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.trajectories import policy_step\n",
    "from tf_agents.utils import common\n",
    "import logging\n",
    "\n",
    "import config\n",
    "from  environments import CardGameEnv\n",
    "from utils import *\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()\n",
    "os.makedirs(config.LOGDIR,exist_ok=True)\n",
    "os.makedirs(config.MODEL_SAVE,exist_ok=True)\n",
    "logging.basicConfig(filename=os.path.join(config.LOGDIR,'log.log'), \n",
    "level=logging.INFO, \n",
    "format='%(asctime)s | %(name)s | %(levelname)s | %(message)s')\n",
    "if __name__=='__main__':\n",
    "    train_py_env = CardGameEnv()\n",
    "    eval_py_env = CardGameEnv()\n",
    "    \n",
    "    train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "    eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
    "    actor_fc_layers = config.actor_fc_layers\n",
    "    critic_obs_fc_layers = config.critic_obs_fc_layers\n",
    "    critic_action_fc_layers = config.critic_action_fc_layers\n",
    "    critic_joint_fc_layers = config.critic_joint_fc_layers\n",
    "    ou_stddev = config.ou_stddev\n",
    "    ou_damping = config.ou_damping\n",
    "    target_update_tau = config.target_update_tau\n",
    "    target_update_period = config.target_update_period\n",
    "    dqda_clipping = config.dqda_clipping\n",
    "    td_errors_loss_fn = config.td_errors_loss_fn\n",
    "    gamma = config.gamma\n",
    "    reward_scale_factor = config.reward_scale_factor\n",
    "    gradient_clipping = config.gradient_clipping\n",
    "\n",
    "    actor_learning_rate = config.actor_learning_rate\n",
    "    critic_learning_rate = config.critic_learning_rate\n",
    "    debug_summaries = config.debug_summaries\n",
    "    summarize_grads_and_vars = config.summarize_grads_and_vars\n",
    "    \n",
    "    global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "    actor_net = actor_network.ActorNetwork(\n",
    "            train_env.time_step_spec().observation,\n",
    "            train_env.action_spec(),\n",
    "            fc_layer_params=actor_fc_layers,\n",
    "        )\n",
    "\n",
    "    critic_net_input_specs = (train_env.time_step_spec().observation,\n",
    "                            train_env.action_spec())\n",
    "\n",
    "    critic_net = critic_network.CriticNetwork(\n",
    "        critic_net_input_specs,\n",
    "        observation_fc_layer_params=critic_obs_fc_layers,\n",
    "        action_fc_layer_params=critic_action_fc_layers,\n",
    "        joint_fc_layer_params=critic_joint_fc_layers,\n",
    "    )\n",
    "\n",
    "    tf_agent = ddpg_agent.DdpgAgent(\n",
    "        train_env.time_step_spec(),\n",
    "        train_env.action_spec(),\n",
    "        actor_network=actor_net,\n",
    "        critic_network=critic_net,\n",
    "        actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "            learning_rate=actor_learning_rate),\n",
    "        critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "            learning_rate=critic_learning_rate),\n",
    "        ou_stddev=ou_stddev,\n",
    "        ou_damping=ou_damping,\n",
    "        target_update_tau=target_update_tau,\n",
    "        target_update_period=target_update_period,\n",
    "        dqda_clipping=dqda_clipping,\n",
    "        td_errors_loss_fn=td_errors_loss_fn,\n",
    "        gamma=gamma,\n",
    "        reward_scale_factor=reward_scale_factor,\n",
    "        gradient_clipping=gradient_clipping,\n",
    "        debug_summaries=debug_summaries,\n",
    "        summarize_grads_and_vars=summarize_grads_and_vars,\n",
    "        train_step_counter=global_step)\n",
    "    tf_agent.initialize()\n",
    "    \n",
    "    random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                    train_env.action_spec())\n",
    "\n",
    "    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "        data_spec=tf_agent.collect_data_spec,\n",
    "        batch_size=train_env.batch_size,\n",
    "        max_length=config.REPLAY_BUFFER_MAX_LENGTH)\n",
    "\n",
    "    collect_data(train_env, random_policy, replay_buffer, steps=100)\n",
    "\n",
    "    dataset = replay_buffer.as_dataset(\n",
    "        num_parallel_calls=3, \n",
    "        sample_batch_size=config.BATCH_SIZE, \n",
    "        num_steps=2).prefetch(3)\n",
    "    \n",
    "    my_policy = tf_agent.collect_policy\n",
    "    saver = PolicySaver(my_policy, batch_size=None)\n",
    "\n",
    "    iterator = iter(dataset)\n",
    "    tf_agent.train = common.function(tf_agent.train)\n",
    "\n",
    "    # Reset the train step\n",
    "    tf_agent.train_step_counter.assign(0)\n",
    "\n",
    "    # Evaluate the agent's policy once before training.\n",
    "    avg_return = compute_avg_return(eval_env, tf_agent.policy, \\\n",
    "                                    config.NUM_EVAL_EPISODES)\n",
    "    returns = [avg_return]\n",
    "    iterations=[0]\n",
    "    for _ in tqdm(range(config.NUM_ITERATIONS),total=config.NUM_ITERATIONS):\n",
    "            # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "            for _ in range(config.COLLECT_STEPS_PER_ITERATION):\n",
    "                collect_step(train_env, tf_agent.collect_policy, replay_buffer)\n",
    "\n",
    "            # Sample a batch of data from the buffer and update the agent's network.\n",
    "            experience, unused_info = next(iterator)\n",
    "            train_loss = tf_agent.train(experience).loss\n",
    "\n",
    "            step = tf_agent.train_step_counter.numpy()\n",
    "\n",
    "            if step % config.LOG_INTERVAL == 0:\n",
    "                print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "            if step % config.EVAL_INTERVAL == 0:\n",
    "                avg_return = compute_avg_return(eval_env, tf_agent.policy, \\\n",
    "                                                config.NUM_EVAL_EPISODES)\n",
    "                print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "                logging.info('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "                returns.append(avg_return)\n",
    "                iterations.append(step)\n",
    "            if step % config.MODEL_SAVE_FREQ == 0:\n",
    "                saver.save(os.path.join(config.MODEL_SAVE,f'policy_step_{step}_gamma.mdl'))\n",
    "                \n",
    "        # except:\n",
    "        #     print(\"error_skipping\")\n",
    "\n",
    "    # iterations = range(0, num_iteratioens + 1, eval_interval)\n",
    "    plt.plot(iterations, returns)\n",
    "    plt.ylabel('Average Return')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylim(top=50)\n",
    "    plt.show()\n",
    "    plt.savefig(\"output_img_gamma.png\")\n",
    "    pd.DataFrame({\"interations\":iterations,\"Return\":returns}).to_csv(os.path.join(config.LOGDIR,\"output_ar_gamma.csv\"),index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
